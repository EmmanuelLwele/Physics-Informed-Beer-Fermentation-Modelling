# ============================================================
# Imports
# ============================================================
import os
import time
import math
import json

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import grad

# Sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

from torch.utils.data import TensorDataset, DataLoader

# ============================================================
# Device & seeds
# ============================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

np.random.seed(42)
torch.manual_seed(42)

# ============================================================
# Load dataset
# ============================================================
file_path = "data/00_PS_IPA_12_SL-23.csv"   # <- adjust if path differs
df = pd.read_csv(file_path)

print("First 5 rows:\n", df.head())
print("\nColumns:\n", df.columns)
print("\nDataset shape:", df.shape)

# If needed, uncomment to inspect column names
# print(df.columns.tolist())

# ============================================================
# Basic preprocessing & plots
# ============================================================
df["Timestamp (BST)"] = pd.to_datetime(df["Timestamp (BST)"])

variables = ["DO (mg/L)", "pH", "Gravity (°P)", "Fluid Temp (°C)"]
yeast_type = "Ale yeast"  # just a label

# Quick overview plots (full duration)
for var in variables:
    plt.figure(figsize=(8, 5))
    plt.plot(df["Hours from Pitch"], df[var], label=var, linewidth=2, color="red", linestyle="-.")
    plt.plot([], [], ' ', label=f"Yeast: {yeast_type}")
    plt.xlabel("Hours from Pitch")
    plt.ylabel(var)
    plt.title(f"{var} over Fermentation Time")
    plt.grid(True, linestyle="--", color="blue", alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.show()

# Limit to ≤ 300 h for descriptive plots
df_cut = df[df["Hours from Pitch"] <= 300]

for var in variables:
    plt.figure(figsize=(8, 5))
    plt.plot(df_cut["Hours from Pitch"], df_cut[var],
             label=var, linewidth=2, color="red", linestyle="-.")
    plt.plot([], [], ' ', label=f"Yeast: {yeast_type}")
    plt.xlabel("Hours from Pitch")
    plt.ylabel(var)
    plt.title(f"{var} over Fermentation Time (First 300 hours)")
    plt.grid(True, linestyle="--", color="blue", alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.show()

# Dataset info
df_info = {
    "shape": df.shape,
    "columns": list(df.columns),
    "dtypes": df.dtypes.apply(lambda x: str(x)).to_dict(),
    "head": df.head(8).applymap(lambda x: str(x) if isinstance(x, pd.Timestamp) else x).to_dict(orient="list"),
    "nan_counts": df.isna().sum().to_dict()
}
print(json.dumps(df_info, indent=2))

# ============================================================
# Extract basic time / Plato / temperature
# ============================================================
t_data = df['Hours from Pitch'].to_numpy(dtype=float)
plato  = df['Gravity (°P)'].to_numpy(dtype=float)
temp   = df['Fluid Temp (°C)'].to_numpy(dtype=float)

t_min, t_max = float(np.min(t_data)), float(np.max(t_data))
print(f"Physical Time Range: {t_min:.2f} h to {t_max:.2f} h")
print(f"Total Fermentation Time: {t_max - t_min:.2f} h")

# ============================================================
# Scaling functions for PINN
# ============================================================
def scale_time(t): 
    return (t - t_min) / (t_max - t_min + 1e-12)

def unscale_time(ts): 
    return ts * (t_max - t_min) + t_min

def scale_temp(T): 
    return (T - np.mean(temp)) / (np.max(temp)-np.min(temp)+1e-12)

t_scaled_all = scale_time(t_data).reshape(-1,1)
T_scaled_all = scale_temp(temp).reshape(-1,1)

# ============================================================
# Plato → Sugar concentration Cs (g/L)
# ============================================================
def plato_to_concentration(P):
    rho = 1 + 0.003867 * P + 0.00001243 * P**2 + 0.000000013 * P**3
    Cs = (P / 100.0) * rho * 1000.0
    return Cs

Cs_data = plato_to_concentration(plato)
print("First 5 sugar concentrations (g/L):", Cs_data[:5])

# Sugar vs time (full)
plt.figure(figsize=(8, 5))
plt.plot(t_data, Cs_data, linewidth=2, color="blue", linestyle="-.")
plt.xlabel("Time (hours)")
plt.ylabel("Sugar Concentration (g/L)")
plt.title("Sugar Concentration (g/L) vs Time (h)")
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Sugar vs time (≤300 h)
mask_300_data = t_data <= 300
plt.figure(figsize=(8, 5))
plt.plot(t_data[mask_300_data], Cs_data[mask_300_data], linewidth=2, color="blue", linestyle="-.")
plt.xlabel("Time (hours)")
plt.ylabel("Sugar Concentration (g/L)")
plt.title("Sugar Concentration (g/L) vs Time (First 300 Hours)")
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# ============================================================
# PINN setup (unchanged logic)
# ============================================================
# Hyperparameters
layers = [2, 128, 128, 128, 3]   # [t_scaled, T_scaled] -> [X, Cs, Ce]
N_f = 3000
lr = 5e-4
adam_epochs = 5000

use_full_haldane = True
Y_e_s_fixed = 0.51

mu_ref = 0.4
K_s_ref = 0.1
K_i_ref = 50.0
T_ref   = 30.0
Ea      = 5000.0
R       = 8.314

w_data = 10
w_phys = 50
w_ic   = 100.0
eps = 1e-8

# PINN tensors
t_Cs   = torch.tensor(t_scaled_all, dtype=torch.float32, device=device, requires_grad=False)
Cs_obs = torch.tensor(Cs_data.reshape(-1, 1), dtype=torch.float32, device=device)
T_input = torch.tensor(T_scaled_all, dtype=torch.float32, device=device)

# Initial conditions
x0  = 0.1
cs0 = float(Cs_data[0])
ce0 = 0.0

# ============================================================
# PINN model definition (unchanged logic)
# ============================================================
class PINN(nn.Module):
    def __init__(self, layers):
        super().__init__()
        self.layers = nn.ModuleList()
        for i in range(len(layers)-1):
            self.layers.append(nn.Linear(layers[i], layers[i+1]))

        # trainable kinetic parameters
        self.p_mu     = nn.Parameter(torch.tensor(0.2))
        self.p_ks     = nn.Parameter(torch.tensor(10.0))
        self.p_ki_s   = nn.Parameter(torch.tensor(100.0))
        self.p_ki_eth = nn.Parameter(torch.tensor(100.0))
        self.p_L      = nn.Parameter(torch.tensor(1.0))
        self.p_Y_xs   = nn.Parameter(torch.tensor(0.1))
        self.p_Q10    = nn.Parameter(torch.tensor(1.5))

    def forward(self, t_scaled, T_scaled):
        x = torch.cat([t_scaled, T_scaled], dim=1)
        for layer in self.layers[:-1]:
            x = torch.tanh(layer(x))
        out = self.layers[-1](x)
        X_hat  = F.softplus(out[:, 0:1]) + 1e-8
        Cs_hat = F.softplus(out[:, 1:2]) + 1e-8
        Ce_hat = F.softplus(out[:, 2:3]) + 1e-8
        return X_hat, Cs_hat, Ce_hat

    def phys_params(self):
        mu_ref  = F.softplus(self.p_mu)     + 1e-8
        ks      = F.softplus(self.p_ks)     + 1e-8
        ki_s    = F.softplus(self.p_ki_s)   + 1e-8
        ki_eth  = F.softplus(self.p_ki_eth) + 1e-8
        L       = F.softplus(self.p_L)      + 1e-8
        Y_xs    = F.softplus(self.p_Y_xs)   + 1e-8
        Q10     = F.softplus(self.p_Q10)    + 1e-8
        return mu_ref, ks, ki_s, ki_eth, L, Y_xs, Q10

    def mu(self, t_phys, Cs, Ce, T_phys, use_full=True):
        mu_ref, ks, ki_s, ki_eth, L, Y_xs, Q10 = self.phys_params()
        T_ref_loc = np.mean(temp)
        temp_factor = Q10 ** ((T_phys - T_ref_loc) / 10.0)

        if use_full:
            denom = ks + Cs + (Cs**2) / (ki_s + 1e-12)
            ethanol_term = 1.0 / (1.0 + Ce / (ki_eth + 1e-12))
            lag = 1.0 - torch.exp(-t_phys / (L + 1e-12))
            mu = mu_ref * temp_factor * (Cs / (denom + 1e-12)) * ethanol_term * lag
        else:
            mu = mu_ref * temp_factor * Cs / (ks + Cs + 1e-12)
        return mu

model = PINN(layers).to(device)
print(f"PINN model on: {device}")

# ============================================================
# Collocation points for physics loss
# ============================================================
t_phys_coll = np.random.uniform(t_min, t_max, size=(N_f, 1))
t_phys_coll_scaled = torch.tensor(
    scale_time(t_phys_coll),
    dtype=torch.float32,
    device=device,
    requires_grad=True
)

T_phys_coll = np.interp(t_phys_coll.flatten(), t_data, temp).reshape(-1, 1)
T_phys_coll_scaled = torch.tensor(
    scale_temp(T_phys_coll),
    dtype=torch.float32,
    device=device,
    requires_grad=False
)

# ============================================================
# Helper for time derivative
# ============================================================
def d_dt(u, t_scaled):
    du = grad(
        outputs=u,
        inputs=t_scaled,
        grad_outputs=torch.ones_like(u),
        create_graph=True,
        retain_graph=True
    )[0]
    scale_factor = 1.0 / (t_max - t_min + 1e-12)
    return du * scale_factor

# ============================================================
# Loss function (unchanged logic)
# ============================================================
def loss_fn():
    X_pred_all, Cs_pred_all, Ce_pred_all = model(t_Cs, T_input)
    data_loss = F.mse_loss(Cs_pred_all, Cs_obs)

    # IC loss
    t0_scaled = torch.tensor([[scale_time(t_min)]], dtype=torch.float32, device=device)
    T0_scaled = torch.tensor([[scale_temp(temp[0])]], dtype=torch.float32, device=device)
    X0_pred, Cs0_pred, Ce0_pred = model(t0_scaled, T0_scaled)

    ic_loss = 0.0
    if cs0 is not None:
        ic_loss += F.mse_loss(Cs0_pred, torch.tensor([[cs0]], dtype=torch.float32, device=device))
    if x0 is not None:
        ic_loss += F.mse_loss(X0_pred, torch.tensor([[x0]], dtype=torch.float32, device=device))
    if ce0 is not None:
        ic_loss += F.mse_loss(Ce0_pred, torch.tensor([[ce0]], dtype=torch.float32, device=device))

    t_phys_scaled = t_phys_coll_scaled
    T_phys_scaled = T_phys_coll_scaled

    t_phys_for_mu = torch.tensor(
        unscale_time(t_phys_scaled.detach().cpu().numpy()),
        dtype=torch.float32,
        device=device
    ).reshape(-1,1)

    t_phys_scaled.requires_grad = True

    Xc, Csc, Cec = model(t_phys_scaled, T_phys_scaled)
    dXdt  = d_dt(Xc,  t_phys_scaled)
    dCsdt = d_dt(Csc, t_phys_scaled)
    dCedt = d_dt(Cec, t_phys_scaled)

    T_phys_for_mu = torch.tensor(
        np.interp(
            t_phys_for_mu.detach().cpu().numpy().flatten(),
            t_data,
            temp
        ).reshape(-1,1),
        dtype=torch.float32,
        device=device
    )

    mu_pred = model.mu(
        t_phys_for_mu,
        Csc,
        Cec,
        T_phys_for_mu,
        use_full=use_full_haldane
    )

    _, _, _, _, _, Y_xs, _ = model.phys_params()

    rX  = dXdt  - mu_pred * Xc
    rCs = dCsdt + (1.0 / (Y_xs + 1e-12)) * mu_pred * Xc
    rCe = dCedt - (Y_e_s_fixed / (Y_xs + 1e-12)) * mu_pred * Xc

    phys_loss = rX.pow(2).mean() + rCs.pow(2).mean() + rCe.pow(2).mean()

    total = w_data * data_loss + w_phys * phys_loss + w_ic * ic_loss
    return total, data_loss.detach(), phys_loss.detach(), ic_loss if isinstance(ic_loss, torch.Tensor) else torch.tensor(ic_loss)

# ============================================================
# PINN training (Adam + LBFGS)
# ============================================================
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1500, gamma=0.7)

start = time.time()
print("Starting Adam training...")

for ep in range(1, adam_epochs + 1):
    optimizer.zero_grad()
    total_loss, dl, pl, il = loss_fn()
    total_loss.backward(retain_graph=True)
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()
    scheduler.step()

    if ep % 250 == 0 or ep == 1:
        mu_ref_t, ks_t, ki_s_t, ki_eth_t, L_t, Y_xs_t, Q10_t = model.phys_params()
        print(f"Epoch {ep}/{adam_epochs}  total={total_loss.item():.6f} "
              f"data={dl.item():.6f} phys={pl.item():.6f} ic={il.item():.6f}")
        print(f"  params: mu_ref={float(mu_ref_t):.4f}, ks={float(ks_t):.3f}, "
              f"ki_s={float(ki_s_t):.1f}, ki_eth={float(ki_eth_t):.1f}, "
              f"L={float(L_t):.3f}, Y_xs={float(Y_xs_t):.4f}, Q10={float(Q10_t):.3f}")

end = time.time()
print("Adam training done in %.1f s" % (end - start))

print("Running short L-BFGS refinement...")
optimizerLBFGS = torch.optim.LBFGS(
    model.parameters(),
    max_iter=150,
    history_size=30,
    line_search_fn='strong_wolfe'
)
def closure():
    optimizerLBFGS.zero_grad()
    total_loss, dl, pl, il = loss_fn()
    total_loss.backward()
    return total_loss

optimizerLBFGS.step(closure)
print("L-BFGS done.")

# ============================================================
# PINN evaluation on dense grid
# ============================================================
model.eval()
with torch.no_grad():
    t_plot = np.linspace(t_min, t_max, 500).reshape(-1,1)
    t_plot_scaled = torch.tensor(scale_time(t_plot), dtype=torch.float32, device=device)
    T_plot = np.interp(t_plot.flatten(), t_data, temp).reshape(-1,1)
    T_plot_scaled = torch.tensor(scale_temp(T_plot), dtype=torch.float32, device=device)

    Xp, Csp, Cep = model(t_plot_scaled, T_plot_scaled)
    Xp  = Xp.cpu().numpy().flatten()
    Csp = Csp.cpu().numpy().flatten()
    Cep = Cep.cpu().numpy().flatten()

# Cs measured vs PINN (full)
plt.figure(figsize=(8,5))
plt.scatter(t_data, Cs_data, s=6, label="Cs measured (from Plato)")
plt.plot(t_plot, Csp, color='red', label="Cs PINN")
plt.xlabel("Time (h)")
plt.ylabel("Cs (g/L)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Cs measured vs PINN (≤ 300 h)
t_data_flat = t_data.ravel()
Cs_data_flat = Cs_data.ravel()
t_plot_flat = t_plot.ravel()
Csp_flat = Csp.ravel()

mask_data_300 = t_data_flat <= 300
mask_plot_300 = t_plot_flat <= 300

t_data_cut = t_data_flat[mask_data_300]
Cs_data_cut = Cs_data_flat[mask_data_300]
t_plot_cut  = t_plot_flat[mask_plot_300]
Csp_cut     = Csp_flat[mask_plot_300]

Cs_pred_interp = np.interp(t_data_cut, t_plot_cut, Csp_cut)
r2 = r2_score(Cs_data_cut, Cs_pred_interp)
rmse = np.sqrt(mean_squared_error(Cs_data_cut, Cs_pred_interp))
mae = mean_absolute_error(Cs_data_cut, Cs_pred_interp)

plt.figure(figsize=(8, 5))
plt.scatter(t_data_cut, Cs_data_cut, s=8, label="Cs measured (from Plato)", color='blue')
plt.plot(t_plot_cut, Csp_cut, color='red', linewidth=2, label="Cs PINN prediction")
textstr = f"R² = {r2:.3f}\nRMSE = {rmse:.3f} g/L\nMAE = {mae:.3f} g/L"
plt.text(0.98, 0.05, textstr, transform=plt.gca().transAxes,
         fontsize=10, verticalalignment='bottom', horizontalalignment='right',
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
plt.xlabel("Time (h)")
plt.ylabel("Cs (g/L)")
plt.title("Sugar Concentration (Cs) — Measured vs PINN (≤300 Hours)")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# X & Ce PINN plots (≤300 h)
t_plot_flat = t_plot_flat
Xp_flat     = Xp.ravel()
Cep_flat    = Cep.ravel()

mask_plot_300 = t_plot_flat <= 300
t_plot_cut = t_plot_flat[mask_plot_300]
Xp_cut    = Xp_flat[mask_plot_300]
Cep_cut   = Cep_flat[mask_plot_300]

plt.figure(figsize=(8,4))
plt.plot(t_plot_cut, Xp_cut, color='green', linewidth=2, label="X PINN")
plt.xlabel("Time (h)")
plt.ylabel("Biomass X (g/L)")
plt.title("Biomass X — PINN Prediction (≤300 h)")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8,4))
plt.plot(t_plot_cut, Cep_cut, color='orange', linewidth=2, label="Ce PINN")
plt.xlabel("Time (h)")
plt.ylabel("Ethanol Ce (g/L)")
plt.title("Ethanol Ce — PINN Prediction (≤300 h)")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ============================================================
# Learned parameters
# ============================================================
mu_ref_t, ks_t, ki_s_t, ki_eth_t, L_t, Y_xs_t, Q10_t = model.phys_params()
print("Learned parameters:")
print(f"mu_ref = {float(mu_ref_t):.5f} h^-1")
print(f"ks     = {float(ks_t):.5f} g/L")
print(f"ki_s   = {float(ki_s_t):.5f} g/L")
print(f"ki_eth = {float(ki_eth_t):.5f} g/L")
print(f"L      = {float(L_t):.5f} h")
print(f"Y_x/s  = {float(Y_xs_t):.5f} g/g")
print(f"Q10    = {float(Q10_t):.5f}")

# ============================================================
# Save PINN results
# ============================================================
Cs_measured_interp = np.interp(t_plot.flatten(), t_data, Cs_data)
out = pd.DataFrame({
    "time_h": t_plot.flatten(),
    "Cs_measured_gL": Cs_measured_interp,
    "Cs_pred_gL": Csp,
    "X_pred_gL": Xp,
    "Ce_pred_gL": Cep
})
out_path = "data/pinn_results_00_PS_IPA_12_SL-23.csv"
out.to_csv(out_path, index=False)
print("Saved PINN predictions to", out_path)

# ============================================================
# SURROGATES: RF / GP / LSTM for Cs and Ce
# ============================================================
print("\n=== Building data-driven surrogates (RF, GP, LSTM) for Cs and Ce ===")

# Features: time + temperature
X_raw = np.column_stack([t_data, temp]).astype(float)
y_Cs  = Cs_data.astype(float)

# Ce target from PINN at measurement times
with torch.no_grad():
    t_meas_scaled = torch.tensor(t_scaled_all, dtype=torch.float32, device=device)
    T_meas_scaled = torch.tensor(T_scaled_all, dtype=torch.float32, device=device)
    _, Cs_meas_PINN, Ce_meas_PINN = model(t_meas_scaled, T_meas_scaled)
    Cs_PINN_meas = Cs_meas_PINN.cpu().numpy().flatten()
    Ce_PINN_meas = Ce_meas_PINN.cpu().numpy().flatten()

y_Ce = Ce_PINN_meas

# Train/test split preserving time order
N = X_raw.shape[0]
split_idx = int(0.8 * N)

X_train_raw, X_test_raw = X_raw[:split_idx], X_raw[split_idx:]
y_Cs_train, y_Cs_test   = y_Cs[:split_idx], y_Cs[split_idx:]
y_Ce_train, y_Ce_test   = y_Ce[:split_idx], y_Ce[split_idx:]

t_test   = t_data[split_idx:]
temp_test= temp[split_idx:]

print(f"Train size: {X_train_raw.shape[0]}, Test size: {X_test_raw.shape[0]}")

# Feature scaling
X_scaler = StandardScaler()
X_train = X_scaler.fit_transform(X_train_raw)
X_test  = X_scaler.transform(X_test_raw)

# ----- LSTM helpers -----
def create_sequences(X, y, seq_len=10):
    xs, ys = [], []
    for i in range(len(X) - seq_len):
        xs.append(X[i:i+seq_len])
        ys.append(y[i+seq_len])
    return np.array(xs), np.array(ys)

seq_len = 10

y_Cs_scaler = StandardScaler()
y_Ce_scaler = StandardScaler()

y_Cs_train_scaled = y_Cs_scaler.fit_transform(y_Cs_train.reshape(-1, 1)).ravel()
y_Cs_test_scaled  = y_Cs_scaler.transform(y_Cs_test.reshape(-1, 1)).ravel()

y_Ce_train_scaled = y_Ce_scaler.fit_transform(y_Ce_train.reshape(-1, 1)).ravel()
y_Ce_test_scaled  = y_Ce_scaler.transform(y_Ce_test.reshape(-1, 1)).ravel()

X_seq_train_Cs, y_seq_Cs_train = create_sequences(X_train, y_Cs_train_scaled, seq_len)
X_seq_test_Cs,  y_seq_Cs_test  = create_sequences(X_test,  y_Cs_test_scaled,  seq_len)

X_seq_train_Ce, y_seq_Ce_train = create_sequences(X_train, y_Ce_train_scaled, seq_len)
X_seq_test_Ce,  y_seq_Ce_test  = create_sequences(X_test,  y_Ce_test_scaled,  seq_len)

print("LSTM Cs train sequences:", X_seq_train_Cs.shape, "test:", X_seq_test_Cs.shape)
print("LSTM Ce train sequences:", X_seq_train_Ce.shape, "test:", X_seq_test_Ce.shape)

# ----- LSTM regressor -----
class LSTMRegressor(nn.Module):
    def __init__(self, input_size=2, hidden_size=64, num_layers=1, dropout=0.0):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0,
        )
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        last_hidden = out[:, -1, :]
        return self.fc(last_hidden)

# ----- Train LSTM for Cs -----
X_seq_train_Cs_t = torch.tensor(X_seq_train_Cs, dtype=torch.float32).to(device)
y_seq_Cs_train_t = torch.tensor(y_seq_Cs_train, dtype=torch.float32).unsqueeze(-1).to(device)
X_seq_test_Cs_t  = torch.tensor(X_seq_test_Cs,  dtype=torch.float32).to(device)
y_seq_Cs_test_t  = torch.tensor(y_seq_Cs_test,  dtype=torch.float32).unsqueeze(-1).to(device)

train_ds_Cs = TensorDataset(X_seq_train_Cs_t, y_seq_Cs_train_t)
train_loader_Cs = DataLoader(train_ds_Cs, batch_size=32, shuffle=True)

lstm_Cs = LSTMRegressor(input_size=X_seq_train_Cs.shape[-1], hidden_size=64).to(device)
crit = nn.MSELoss()
opt = torch.optim.Adam(lstm_Cs.parameters(), lr=1e-3)

epochs = 200
for epoch in range(1, epochs + 1):
    lstm_Cs.train()
    running = 0.0
    for xb, yb in train_loader_Cs:
        opt.zero_grad()
        preds = lstm_Cs(xb)
        loss = crit(preds, yb)
        loss.backward()
        opt.step()
        running += loss.item() * xb.size(0)
    if epoch % 50 == 0 or epoch == 1:
        print(f"[LSTM Cs] Epoch {epoch}/{epochs}, MSE={running/len(train_ds_Cs):.6f}")

# ----- Train LSTM for Ce -----
X_seq_train_Ce_t = torch.tensor(X_seq_train_Ce, dtype=torch.float32).to(device)
y_seq_Ce_train_t = torch.tensor(y_seq_Ce_train, dtype=torch.float32).unsqueeze(-1).to(device)
X_seq_test_Ce_t  = torch.tensor(X_seq_test_Ce,  dtype=torch.float32).to(device)
y_seq_Ce_test_t  = torch.tensor(y_seq_Ce_test,  dtype=torch.float32).unsqueeze(-1).to(device)

train_ds_Ce = TensorDataset(X_seq_train_Ce_t, y_seq_Ce_train_t)
train_loader_Ce = DataLoader(train_ds_Ce, batch_size=32, shuffle=True)

lstm_Ce = LSTMRegressor(input_size=X_seq_train_Ce.shape[-1], hidden_size=64).to(device)
opt_Ce = torch.optim.Adam(lstm_Ce.parameters(), lr=1e-3)

for epoch in range(1, epochs + 1):
    lstm_Ce.train()
    running = 0.0
    for xb, yb in train_loader_Ce:
        opt_Ce.zero_grad()
        preds = lstm_Ce(xb)
        loss = crit(preds, yb)
        loss.backward()
        opt_Ce.step()
        running += loss.item() * xb.size(0)
    if epoch % 50 == 0 or epoch == 1:
        print(f"[LSTM Ce] Epoch {epoch}/{epochs}, MSE={running/len(train_ds_Ce):.6f}")

# ----- RF models -----
rf_Cs = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
rf_Cs.fit(X_train, y_Cs_train)
y_Cs_rf_test = rf_Cs.predict(X_test)

rf_Ce = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
rf_Ce.fit(X_train, y_Ce_train)
y_Ce_rf_test = rf_Ce.predict(X_test)

# ----- GP models -----
kernel = 1.0 * RBF(length_scale=[10.0, 5.0], length_scale_bounds=(1e-2, 1e3)) \
         + WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-5, 1e5))

gp_Cs = GaussianProcessRegressor(kernel=kernel, normalize_y=True,
                                 n_restarts_optimizer=3, random_state=42)
gp_Cs.fit(X_train, y_Cs_train)
y_Cs_gp_test, _ = gp_Cs.predict(X_test, return_std=True)

gp_Ce = GaussianProcessRegressor(kernel=kernel, normalize_y=True,
                                 n_restarts_optimizer=3, random_state=42)
gp_Ce.fit(X_train, y_Ce_train)
y_Ce_gp_test, _ = gp_Ce.predict(X_test, return_std=True)

# ----- LSTM predictions (inverse scaling) -----
lstm_Cs.eval()
with torch.no_grad():
    y_Cs_lstm_test_scaled = lstm_Cs(X_seq_test_Cs_t).cpu().numpy().squeeze()
y_Cs_lstm_test = y_Cs_scaler.inverse_transform(
    y_Cs_lstm_test_scaled.reshape(-1, 1)
).ravel()

lstm_Ce.eval()
with torch.no_grad():
    y_Ce_lstm_test_scaled = lstm_Ce(X_seq_test_Ce_t).cpu().numpy().squeeze()
y_Ce_lstm_test = y_Ce_scaler.inverse_transform(
    y_Ce_lstm_test_scaled.reshape(-1, 1)
).ravel()

# ============================================================
# PINN predictions on the TEST portion (for comparison)
# ============================================================
with torch.no_grad():
    t_test_scaled = torch.tensor(
        scale_time(t_test).reshape(-1, 1),
        dtype=torch.float32,
        device=device
    )
    T_test_scaled = torch.tensor(
        scale_temp(temp_test).reshape(-1, 1),
        dtype=torch.float32,
        device=device
    )
    X_test_PINN, Cs_test_PINN, Ce_test_PINN = model(t_test_scaled, T_test_scaled)
    Cs_PINN_test = Cs_test_PINN.cpu().numpy().flatten()
    Ce_PINN_test = Ce_test_PINN.cpu().numpy().flatten()

# ============================================================
# Align arrays for LSTM (shorter by seq_len) + restrict to ≤300 h
# ============================================================
offset = seq_len

t_plot_test = t_test[offset:]  # time axis where LSTM has predictions

# Cs
Cs_true_plot = y_Cs_test[offset:]
Cs_PINN_plot = Cs_PINN_test[offset:]
Cs_rf_plot   = y_Cs_rf_test[offset:]
Cs_gp_plot   = y_Cs_gp_test[offset:]
Cs_lstm_plot = y_Cs_lstm_test

# Ce (PINN used as reference)
Ce_true_plot = Ce_PINN_test[offset:]
Ce_PINN_plot = Ce_PINN_test[offset:]
Ce_rf_plot   = y_Ce_rf_test[offset:]
Ce_gp_plot   = y_Ce_gp_test[offset:]
Ce_lstm_plot = y_Ce_lstm_test

# Restrict ALL to t ≤ 300 h
mask_300 = t_plot_test <= 300.0
t_plot_test_300 = t_plot_test[mask_300]

Cs_true_plot_300 = Cs_true_plot[mask_300]
Cs_PINN_plot_300 = Cs_PINN_plot[mask_300]
Cs_rf_plot_300   = Cs_rf_plot[mask_300]
Cs_gp_plot_300   = Cs_gp_plot[mask_300]
Cs_lstm_plot_300 = Cs_lstm_plot[mask_300]

Ce_true_plot_300 = Ce_true_plot[mask_300]
Ce_PINN_plot_300 = Ce_PINN_plot[mask_300]
Ce_rf_plot_300   = Ce_rf_plot[mask_300]
Ce_gp_plot_300   = Ce_gp_plot[mask_300]
Ce_lstm_plot_300 = Ce_lstm_plot[mask_300]

# ============================================================
# FINAL COMPARISON PLOTS (≤300 h)
# ============================================================
# Cs comparison
plt.figure(figsize=(8, 5))
plt.plot(t_plot_test_300, Cs_true_plot_300, label="Cs measured (test)", linewidth=2)
plt.plot(t_plot_test_300, Cs_PINN_plot_300, label="Cs PINN", linestyle="-")
plt.plot(t_plot_test_300, Cs_rf_plot_300,   label="Cs RF",   linestyle="--")
plt.plot(t_plot_test_300, Cs_gp_plot_300,   label="Cs GP",   linestyle="-.")
plt.plot(t_plot_test_300, Cs_lstm_plot_300, label="Cs LSTM", linestyle=":")

plt.xlabel("Time (h)")
plt.ylabel("Cs (g/L)")
plt.title("Cs – PINN vs RF vs GP vs LSTM (test set, ≤ 300 h)")
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()

# Ce comparison (PINN as reference)
plt.figure(figsize=(8, 5))
plt.plot(t_plot_test_300, Ce_PINN_plot_300, label="Ce PINN (reference)", linewidth=2)
plt.plot(t_plot_test_300, Ce_rf_plot_300,   label="Ce RF",   linestyle="--")
plt.plot(t_plot_test_300, Ce_gp_plot_300,   label="Ce GP",   linestyle="-.")
plt.plot(t_plot_test_300, Ce_lstm_plot_300, label="Ce LSTM", linestyle=":")

plt.xlabel("Time (h)")
plt.ylabel("Ce (g/L)")
plt.title("Ce – PINN vs RF vs GP vs LSTM surrogates (test set, ≤ 300 h)")
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()
